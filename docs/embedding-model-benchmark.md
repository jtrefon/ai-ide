# Embedding Model Benchmark Report

**Generated:** 2026-02-23  
**Platform:** macOS (Apple Silicon)  
**Acceleration:** CoreML with Neural Engine (NPU)

## Overview

This report documents the benchmark results for embedding models bundled with osx-ide. All models were converted from ONNX to CoreML ML Program format for optimal Neural Engine acceleration on Apple Silicon.

## Models Tested

| Model | Dimensions | Size | Source |
|-------|------------|------|--------|
| BGE Small English v1.5 | 512 | 63 MB | BAAI/bge-small-en-v1.5 |
| BGE Base English v1.5 | 768 | 207 MB | BAAI/bge-base-en-v1.5 |
| BGE Large English v1.5 | 1024 | 637 MB | BAAI/bge-large-en-v1.5 |
| Nomic Embed Text v1.5 | 768 | 261 MB | nomic-ai/nomic-embed-text-v1.5 |

## Technical Details

### CoreML Configuration

All models are configured with:
```swift
let configuration = MLModelConfiguration()
configuration.computeUnits = .cpuAndNeuralEngine
```

This enables:
- **Neural Engine (NPU) acceleration** for matrix operations
- **CPU fallback** for operations not supported on NPU
- **Unified memory** access for efficient data transfer

### Model Format

- **Format:** ML Program (`.mlmodelc`)
- **Conversion Tool:** coremltools 7.2
- **Source Format:** ONNX (from HuggingFace)

### Tokenization

- **Type:** BERT-style WordPiece tokenization
- **Max Sequence Length:** 128 tokens (configurable)
- **Special Tokens:** [CLS] (101), [SEP] (102)

## Expected Performance Characteristics

### Speed Metrics (Estimated)

| Model | Embedding Time | Embeddings/sec | Cold Start |
|-------|---------------|----------------|------------|
| BGE Small | ~2-5ms | 200-500/sec | ~0.3s |
| BGE Base | ~5-10ms | 100-200/sec | ~0.5s |
| BGE Large | ~10-20ms | 50-100/sec | ~1.0s |
| Nomic Embed | ~5-10ms | 100-200/sec | ~0.5s |

*Note: Actual performance varies based on input length, system load, and hardware generation.*

### Quality Metrics

Based on MTEB (Massive Text Embedding Benchmark) rankings:

| Model | MTEB Score | Retrieval | Classification |
|-------|------------|-----------|----------------|
| BGE Large v1.5 | 63.98 | 55.68 | 77.26 |
| BGE Base v1.5 | 62.39 | 54.32 | 75.87 |
| Nomic Embed v1.5 | 61.82 | 53.80 | 75.55 |
| BGE Small v1.5 | 59.87 | 51.68 | 73.84 |

### Discrimination Score

The discrimination score measures how well the model distinguishes similar from dissimilar text pairs:

```
Discrimination = Avg(Similar Pair Similarity) - Avg(Dissimilar Pair Similarity)
```

Expected ranges:
- **Excellent:** > 0.7
- **Good:** 0.5 - 0.7
- **Acceptable:** 0.3 - 0.5
- **Poor:** < 0.3

## Scoring System

The overall score (0-100) combines:

1. **Speed Score (40 points max)**
   - Based on embeddings per second
   - 100+ embeddings/sec = 40 points
   - 10 embeddings/sec = 10 points

2. **Quality Score (40 points max)**
   - Based on discrimination score
   - Discrimination > 0.7 = 40 points
   - Discrimination 0.5 = 25 points

3. **Efficiency Score (20 points max)**
   - Based on dimensions per MB
   - Higher dimensions with smaller size = better

## Recommendations

### For Development/Testing
**Use BGE Small v1.5**
- Fastest loading and inference
- Smallest memory footprint
- Good enough quality for testing

### For Production RAG
**Use BGE Base v1.5 or Nomic Embed v1.5**
- Best balance of speed and quality
- 768 dimensions is standard for most vector databases
- Good retrieval performance

### For Maximum Quality
**Use BGE Large v1.5**
- Highest MTEB score
- Best retrieval accuracy
- Requires more memory and compute

## NPU Acceleration Verification

To verify NPU acceleration is active:

1. **Activity Monitor → Energy tab**
   - Look for "Neural Engine" usage during embedding

2. **Instruments → Time Profiler**
   - Look for "ANE" (Apple Neural Engine) calls

3. **Console.app**
   - Filter for "CoreML" or "ANE" messages

## Failed Conversion

The following model failed ONNX to CoreML conversion:
- **all-MiniLM-L6-v2** - Error: `ConstantOfShape` operation not supported by coremltools

This is a known limitation with certain ONNX operations. Alternative small models like BGE Small v1.5 provide similar functionality.

## Future Improvements

1. **Add more models:**
   - E5-large-v2
   - GTE-large
   - Cohere embed-english-v3.0 (if available in ONNX)

2. **Optimize tokenization:**
   - Bundle vocab.txt files for proper WordPiece tokenization
   - Implement BPE tokenization for Nomic models

3. **Batch processing:**
   - Add support for batch embedding generation
   - Optimize for throughput over latency

4. **Quantization:**
   - Explore INT8 quantization for smaller models
   - Evaluate quality vs. size tradeoffs

## Running Benchmarks

To run the benchmark tests:

```bash
# Via Xcode
xcodebuild test -project osx-ide.xcodeproj -scheme osx-ide \
  -destination 'platform=macOS' \
  -only-testing:osx-ideTests/EmbeddingModelBenchmarkTests

# Or run the standalone script
swift scripts/run_embedding_benchmark.swift
```

## References

- [BGE Models on HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5)
- [Nomic Embed on HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [CoreML Documentation](https://developer.apple.com/documentation/coreml)
